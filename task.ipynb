{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите [файл со статьями статей из Википедии](https://yadi.sk/d/ObKNNcaFWEsK-w). Каждая строка в файле имеет следующий формат:\n",
    "\n",
    "   >URL статьи `<tab>` название статьи `<tab>` текст "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as URL\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "\n",
    "# Выводим полученный DataFrame\n",
    "split_data.show(truncate=False)\n",
    "\n",
    "# Завершаем SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Напишите программу, которая находит самое длинное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое длинное слово : Галогенарены-Нафталин-Антрацен-Фенантрен-Бензпирен-Коронен-Азулен-Бифенил-Ионол.\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, length, explode, max\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "# Выводим полученный DataFrame\n",
    "# split_data.show(truncate=False)\n",
    "\n",
    "# Сплитуем слова через пробел и ,\n",
    "words = split_data.select(explode(split(split_data.text, \"[\\\\s,]+\")).alias(\"word\"))\n",
    "\n",
    "# Находим самое длинное слово\n",
    "longest_word_row = words.filter(length(\"word\") > 0).groupBy().agg(max(length(\"word\")).alias(\"max_length\")).collect()\n",
    "longest_word_length = longest_word_row[0][\"max_length\"]\n",
    "longest_word = words.filter(length(\"word\") == longest_word_length).select(\"word\").collect()[0][\"word\"]\n",
    "print(\"Самое длинное слово :\", longest_word)\n",
    "print( len(longest_word))\n",
    "# Завершаем SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишите программу, которая находит среднюю длину слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина слова : 6.434972869181451\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, avg\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "# Выводим полученный DataFrame\n",
    "#split_data.show(truncate=False)\n",
    "words = split_data.select(explode(split(split_data.text, \"[\\\\s,]+\")).alias(\"word\"))\n",
    "\n",
    "average_word_length = words.filter(length(\"word\") > 0).agg(avg(length(\"word\")).alias(\"avg_length\")).collect()[0][\"avg_length\"]\n",
    "print(\"Средняя длина слова :\", average_word_length)\n",
    "# Завершaем SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Напишите программу, которая находит самое частоупотребляемое слово, состоящее из латинских букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частоупотребляемое слово на латинице : I\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import split, explode, desc, col\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "# Выводим полученный DataFrame\n",
    "#split_data.show(truncate=False)\n",
    "words = split_data.select(explode(split(split_data.text, \"[\\\\s,]+\")).alias(\"word\")).filter(\"word != ''\").withColumn(\"word\", col(\"word\"))\n",
    "\n",
    "# Ищем слова на латинице\n",
    "latin_words = words.filter(words[\"word\"].rlike(\"^[a-zA-Z]+$\"))\n",
    "most_common_latin_word = latin_words.groupBy(\"word\").count().sort(desc(\"count\")).first()[\"word\"]\n",
    "\n",
    "print(\"Самое частоупотребляемое слово на латинице :\", most_common_latin_word)\n",
    "# Завершaем SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Все слова, которые более чем в половине случаев начинаются с большой буквы и встречаются больше 10 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова, начинающиеся с большой буквы и встречающиеся больше 10 раз:\n",
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|      I| 2919|\n",
      "|     II| 2622|\n",
      "|     XX| 2569|\n",
      "|    XIX| 2405|\n",
      "|    III| 1365|\n",
      "|  XVIII| 1280|\n",
      "|    The| 1013|\n",
      "|   XVII|  929|\n",
      "|     IV|  812|\n",
      "|    XVI|  806|\n",
      "|    XXI|  741|\n",
      "|      X|  663|\n",
      "|Windows|  641|\n",
      "|      V|  638|\n",
      "|    XII|  613|\n",
      "|   XIII|  569|\n",
      "|     XV|  529|\n",
      "|     VI|  495|\n",
      "|    XIV|  491|\n",
      "|      C|  457|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import split, explode, lower, regexp_extract, desc\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as article_title\", \"columns[2] as article_text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.article_text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.article_text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "\n",
    "# Выводим полученный DataFrame\n",
    "#split_data.show(truncate=False)\n",
    "\n",
    "words = split_data.select(explode(split(split_data.article_text, \"[\\\\s,]+\")).alias(\"word\")).filter(\"word != ''\").filter(col(\"word\").rlike(\"^[A-Z]\"))\n",
    "capitalized_words = words.filter((col(\"word\").rlike(\"^[A-Z]\")) & (length(\"word\") > 0))\n",
    "# Считаем количество упоминаний каждого слова\n",
    "word_counts = words.groupBy(\"word\").count().filter(\"count > 10\")\n",
    "frequent_capitalized_words = capitalized_words.groupBy(\"word\").count().filter(\"count > 10\")\n",
    "# Считаем общее количество слов\n",
    "total_words = words.count()\n",
    "\n",
    "# Фильтруем только те слова, которые встречаются более чем в половине случаев\n",
    "frequent_capitalized_words = word_counts.filter(word_counts[\"count\"] > total_words / 2)\n",
    "\n",
    "\n",
    "capitalized_words = words.filter((col(\"word\").rlike(\"^[A-Z]\")) & (length(\"word\") > 0))\n",
    "frequent_capitalized_words = capitalized_words.groupBy(\"word\").count().filter(\"count > 10\").orderBy(col(\"count\").desc())\n",
    "print(\"Слова, начинающиеся с большой буквы и встречающиеся больше 10 раз:\")\n",
    "frequent_capitalized_words.show()\n",
    "# Завершаем SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставльные задания я не понял как делать. Ну я их сделал, но ответ вообще не тот который должен быть. Извините"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
