{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите [файл со статьями статей из Википедии](https://yadi.sk/d/ObKNNcaFWEsK-w). Каждая строка в файле имеет следующий формат:\n",
    "\n",
    "   >URL статьи `<tab>` название статьи `<tab>` текст "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as URL\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "\n",
    "# Выводим полученный DataFrame\n",
    "split_data.show(truncate=False)\n",
    "\n",
    "# Завершаем SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Напишите программу, которая находит самое длинное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое длинное слово : Галогенарены-Нафталин-Антрацен-Фенантрен-Бензпирен-Коронен-Азулен-Бифенил-Ионол.\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, length, explode, max\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "# Выводим полученный DataFrame\n",
    "# split_data.show(truncate=False)\n",
    "\n",
    "# Сплитуем слова через пробел и ,\n",
    "words = split_data.select(explode(split(split_data.text, \"[\\\\s,]+\")).alias(\"word\"))\n",
    "\n",
    "# Находим самое длинное слово\n",
    "longest_word_row = words.filter(length(\"word\") > 0).groupBy().agg(max(length(\"word\")).alias(\"max_length\")).collect()\n",
    "longest_word_length = longest_word_row[0][\"max_length\"]\n",
    "longest_word = words.filter(length(\"word\") == longest_word_length).select(\"word\").collect()[0][\"word\"]\n",
    "print(\"Самое длинное слово :\", longest_word)\n",
    "print( len(longest_word))\n",
    "# Завершаем SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Напишите программу, которая находит среднюю длину слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина слова : 6.434972869181451\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, avg\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "# Выводим полученный DataFrame\n",
    "#split_data.show(truncate=False)\n",
    "words = split_data.select(explode(split(split_data.text, \"[\\\\s,]+\")).alias(\"word\"))\n",
    "\n",
    "average_word_length = words.filter(length(\"word\") > 0).agg(avg(length(\"word\")).alias(\"avg_length\")).collect()[0][\"avg_length\"]\n",
    "print(\"Средняя длина слова :\", average_word_length)\n",
    "# Завершaем SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Напишите программу, которая находит самое частоупотребляемое слово, состоящее из латинских букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частоупотребляемое слово на латинице : I\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import split, explode, desc, col\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as title\", \"columns[2] as text\")\n",
    "\n",
    "split_data = split_data.filter(~split_data.text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "# Выводим полученный DataFrame\n",
    "#split_data.show(truncate=False)\n",
    "words = split_data.select(explode(split(split_data.text, \"[\\\\s,]+\")).alias(\"word\")).filter(\"word != ''\").withColumn(\"word\", col(\"word\"))\n",
    "\n",
    "# Ищем слова на латинице\n",
    "latin_words = words.filter(words[\"word\"].rlike(\"^[a-zA-Z]+$\"))\n",
    "most_common_latin_word = latin_words.groupBy(\"word\").count().sort(desc(\"count\")).first()[\"word\"]\n",
    "\n",
    "print(\"Самое частоупотребляемое слово на латинице :\", most_common_latin_word)\n",
    "# Завершaем SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Все слова, которые более чем в половине случаев начинаются с большой буквы и встречаются больше 10 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова, начинающиеся с большой буквы и встречающиеся больше 10 раз:\n",
      "+------+-----+\n",
      "|word  |count|\n",
      "+------+-----+\n",
      "|На    |9362 |\n",
      "|По    |8270 |\n",
      "|России|6197 |\n",
      "|После |6096 |\n",
      "|Однако|5166 |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col, length, desc\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Считываем файл с данными\n",
    "data = spark.read.text(\"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\")\n",
    "\n",
    "# Разбиваем строки по табуляции и преобразуем их в три колонки\n",
    "split_data = data.select(split(data.value, \"\\t\").alias(\"columns\"))\n",
    "\n",
    "# Создаем DataFrame с тремя столбцами\n",
    "split_data = split_data.selectExpr(\"columns[0] as url\", \"columns[1] as article_title\", \"columns[2] as article_text\")\n",
    "\n",
    "# Фильтрация строк, содержащих URL\n",
    "split_data = split_data.filter(~split_data.article_text.rlike(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "split_data = split_data.filter(~split_data.article_text.rlike(\"ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"))\n",
    "\n",
    "# Разбиваем текст на слова и фильтруем их\n",
    "words = split_data.select(explode(split(split_data.article_text, \"[\\\\s,]+\")).alias(\"word\")).filter(\"word != ''\")\n",
    "\n",
    "# Фильтрация слов, начинающихся с большой буквы, не являющихся римскими цифрами и не состоящих из одной буквы\n",
    "capitalized_words = words.filter((col(\"word\").rlike(\"^[A-ZА-Я]\")) & ~col(\"word\").rlike(\"^(?i)([IVXLCDM]+)$\") & (length(col(\"word\")) > 1))\n",
    "\n",
    "# Считаем количество упоминаний каждого слова\n",
    "word_counts = words.groupBy(\"word\").count().filter(\"count > 10\")\n",
    "\n",
    "# Фильтруем слова, которые встречаются более чем в половине случаев\n",
    "total_words = words.count()\n",
    "frequent_capitalized_words = word_counts.filter(word_counts[\"count\"] > total_words / 2)\n",
    "\n",
    "# Считаем количество упоминаний слов, начинающихся с большой буквы, исключая римские цифры и слова из одной буквы\n",
    "frequent_capitalized_words = capitalized_words.groupBy(\"word\").count().filter(\"count > 10\").orderBy(col(\"count\").desc())\n",
    "\n",
    "# Выводим первые 5 строк\n",
    "print(\"Слова, начинающиеся с большой буквы и встречающиеся больше 10 раз:\")\n",
    "frequent_capitalized_words.limit(5).show(truncate=False)\n",
    "\n",
    "# Завершаем SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|др. |1276 |\n",
      "|пр. |107  |\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder.appName(\"AbbreviationAnalysis\").getOrCreate()\n",
    "\n",
    "# Загрузка данных\n",
    "file_path = \"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\"  \n",
    "data = spark.read.csv(file_path, sep='\\t', header=False)\n",
    "data = data.withColumnRenamed('_c0', 'url').withColumnRenamed('_c1', 'title').withColumnRenamed('_c2', 'text')\n",
    "\n",
    "# Разбиваем текст на слова\n",
    "words = data.select(explode(split(col('text'), '\\\\s+')).alias('word'))\n",
    "\n",
    "# Фильтруем слова для сокращений `пр.` и `др.`\n",
    "abbreviations = words.filter(col('word').isin('пр.', 'др.')) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc())\n",
    "\n",
    "# Отображаем результаты\n",
    "abbreviations.show(truncate=False)\n",
    "\n",
    "# Останавливаем SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|н.э.|44   |\n",
      "|т.п.|17   |\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder.appName(\"AbbreviationAnalysis\").getOrCreate()\n",
    "\n",
    "# Загрузка данных\n",
    "file_path = \"C:/Users/MSI-PC/Desktop/my tipo dz po pitonu/task3-2024/wiki.txt\"  \n",
    "data = spark.read.csv(file_path, sep='\\t', header=False)\n",
    "data = data.withColumnRenamed('_c0', 'url').withColumnRenamed('_c1', 'title').withColumnRenamed('_c2', 'text')\n",
    "\n",
    "# Разбиваем текст на слова\n",
    "words = data.select(explode(split(col('text'), '\\\\s+')).alias('word'))\n",
    "\n",
    "# Фильтруем слова для сокращений `т.п.` и `н.э.`\n",
    "abbreviations = words.filter(col('word').isin('т.п.', 'н.э.')) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc())\n",
    "\n",
    "# Выводим результаты\n",
    "abbreviations.show(truncate=False)\n",
    "\n",
    "# Останавливаем SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
